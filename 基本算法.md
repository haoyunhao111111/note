# 线性回归

- 利用原理来求线性回归

```python
f=open(r'C:\Users\hyh\Desktop\data.txt')
data=[]
for line in f:
    d=list(map(float,line.split('\t')))
    data.append(d)

x1=[]
y1=[]
for i in data:
    x1.append(i[0])
    y1.append(i[1])


plt.plot(x1,y1,'b.')
# plt.show()

M=len(data)
N=len(data[0])

def f(theta,x):
    y=0
    for i in range(N-1):
        y+=theta[i]*x[i]
    y+=theta[N-1]
    return y

#求梯度函数
def grad(data,theta,j):
    g=0
    for i in range(M):
        h = f(theta,data[i])
        if j!=N-1:
            g+=(h-data[i][N-1])*data[i][j]
        else:
            g+=(h-data[i][N-1])
    return g

#求目标函数
def loss(data,theta):
    l=0
    for i in range(M):
        h = f(theta, data(i))
        l+=(h-data[i][N-1])**2
    return l/2


theta = [0 for i in range(N)]
theta_new = [0 for i in range(N)]
g = [0 for i in range(N)]
times = 0
alpha = 0.001
while times < 1000000:
    flag = 0
    for j in range(N):
        g = grad(data,theta,j)
        theta_new[j] = theta[j] - alpha * g
        if abs(theta_new[j] - theta[j]) < 0.000001:
            flag = 1
            break
        else:
            theta[j] = theta_new[j]
            times += 1
    if flag:
        break


#找数据集最大值，最小值，做回归线
x2=[min(x1),max(x1)]
y2=[theta[0]*x2[0]+theta[1],theta[0]*x2[1]+theta[1]]
plt.plot(x2,y2,'r--')
plt.show()
```

- 用数学思想来求

```python
data=np.loadtxt(r'C:\Users\hyh\Desktop\data.txt')   # loadtxt直接把数据转化为矩阵形式
m,n=data.shape
X=np.ones((m,n))
X[:,1:]=data[:,0:-1]
y=data[:,-1]
alpha=0.001
theta=[0,0]
while True:
    h=np.dot(X,theta)
    g = np.dot(X.T,(h - y))
    theta_new=theta- alpha * g
    if abs(theta_new-theta).all()<0.000001:
        break
    else:
        theta=theta_new

x=data[:,0:-1]
plt.plot(x,y,'b.')
x2=[min(x),max(x)]
y2=[theta[1]*x2[0]+theta[0],theta[1]*x2[1]+theta[0]]
plt.plot(x2,y2,'r-')
plt.show()

th=np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)

from sklearn.linear_model import LinearRegression
reg=LinearRegression()
reg=reg.fit(data[:,0].reshape(-1,1),data[:,1])
xie=reg.coef_   #斜率
jie=reg.intercept_  #截距
print(xie,jie)

```

# 逻辑回归

```python
data=np.loadtxt(r'C:\Users\hyh\Desktop\logistic.txt')
x1=[]
x2=[]
y1=[]
y2=[]


#数据进行分类
for item in data:
    if item[2]==0.0:
        x1.append(item[0])
        y1.append(item[1])
    else:
        x2.append(item[0])
        y2.append(item[1])


m,n=np.shape(data)
X=np.ones((m,n))
X[:,:-1]=data[:,:-1]
y=data[:,-1]

def sigmoid(x):
    return 1/(1+np.exp(-x))

alpha=0.001
theta=np.ones(n)
while True:
     h=sigmoid(np.dot(X,theta))
     g=np.dot(X.T,(h-y))
     theta_new=theta-alpha*g
     if (abs(theta-theta_new).all()<0.001):
         break
     else:
         theta=theta_new
plt.plot(x1,y1,'r.')
plt.plot(x2,y2,'b.')
x=np.arange(-3,4)
y=-(theta[0]/theta[1])*x-(theta[2]/theta[1])
plt.plot(x,y,'g--')

aa=np.dot([5,4,1],theta)
bb=sigmoid(aa)
print(bb)

from sklearn.linear_model import LogisticRegression
clf=LogisticRegression()
clf.fit(data[:,:-1],data[:,-1])
xie=clf.coef_
jie=clf.intercept_
x=np.arange(-3,4)
y=-(xie[0][0]/xie[0][1])*x-(jie/xie[0][1])
plt.plot(x,y,'g--')
plt.show()
```

# 朴素贝叶斯

- 原理

```python
import numpy as np
def loadData():
    wordList=[['my','name','is','David'],
                 ['you','are','stupid'],
                 ['my','boyfriend','is','SB'],
                 ['you','looks','very','smart','I','like','you','very','much']]
    classList=[0,1,1,0]
    return wordList,classList

def creatVocabList(wordList):
    vocabSet=set([])
    for document in wordList:
        vocabSet=vocabSet|set(document)
    vocabList=list(vocabSet)
    return vocabList


#构建词集模型   把每段的词转化为向量
def setOfWords2Vec(vocabList,words):
    wordVec=[0]*len(vocabList)
    for word in words:
        if word in vocabList:
            wordVec[vocabList.index(word)]=1
    return wordVec

def bagOfWords2Vec(vocabList,words):
    wordVec=[0]*len(vocabList)
    for word in words:
        if word in vocabList:
            wordVec[vocabList.index(word)]+=1
    return wordVec

wordList,classList=loadData()
vocabList=creatVocabList(wordList)
trainMat=[]

for words in wordList:
    trainMat.append(setOfWords2Vec(vocabList,words))

def trainNB(trainMat,classList):
    numWords=len(vocabList)
    pSpam=(sum(classList)+1)/(len(classList)+2)
    p1Num=np.ones(numWords)
    p0Num=np.ones(numWords)
    p1Denom=0
    p0Denom=0
    for i in range(len(classList)):
        if classList[i]==1:
            p1Num+=trainMat[i]
            p1Denom+=sum(trainMat[i])
        else:
            p0Num+=trainMat[i]
            p0Denom+=sum(trainMat[i])
    p1Denom+=numWords
    p0Denom+=numWords
    p1Vec=np.log(p1Num/p1Denom)
    p0Vec=np.log(p0Num/p0Denom)
    return p1Vec,p0Vec,pSpam

def classifyNB(newWordVec,classList):
    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)
    p1=sum(newWordVec*p1Vec)+np.log(pSpam)
    p0=sum(newWordVec*p0Vec)+np.log(1-pSpam)
    if p1>p0:
        return ('粗鲁')
    else:
        return ('文明')

testWords=['my','name','is','a']
newWordVec=setOfWords2Vec(vocabList,testWords)
print(classifyNB(newWordVec,classList))
```

- 英文垃圾邮件分类

```python
import re
import numpy as np

def textParse(bigString):
    line = re.split('\W',bigString)
    return [tokens.lower() for tokens in line if len(tokens) > 2]

wordList = []
classList = []

for i in range(1,26):

    wordList_s =  textParse(open(r'C:\Users\hyh\Desktop\email-英文\spam\%d.txt'%i).read())
    wordList.append(wordList_s)
    classList.append(1)

    wordList_h = textParse(open(r'C:\Users\hyh\Desktop\email-英文\health\%d.txt'%i).read())
    wordList.append(wordList_h)
    classList.append(0)


def creatVocabList(wordList):
    vocabSet=set([])
    for document in wordList:
        vocabSet=vocabSet|set(document)
    vocabList=list(vocabSet)
    return vocabList


#构建词集模型   把每段的词转化为向量
def setOfWords2Vec(vocabList,words):
    wordVec=[0]*len(vocabList)
    for word in words:
        if word in vocabList:
            wordVec[vocabList.index(word)]=1
    return wordVec


trainMat=[]
vocabList=creatVocabList(wordList)
for words in wordList:
    trainMat.append(setOfWords2Vec(vocabList,words))




from sklearn.naive_bayes import GaussianNB,MultinomialNB
clf=MultinomialNB()
clf.fit(trainMat,classList)
textword=textParse(open(r'C:\Users\hyh\Desktop\email-英文\spam\1.txt').read())
newword=setOfWords2Vec(vocabList,textword)
result=clf.predict(np.array(newword).reshape(1,-1))
print(result)

```

- 中文垃圾邮件分类

```python
import jieba
import re
import numpy as np


def creatVocabList(wordList):
    vocabSet = set([])
    for document in wordList:
        vocabSet = vocabSet | set(document)
    vocabList = list(vocabSet)
    return vocabList


def setOfWords2Vec(vocabList, words):
    wordVec = [0] * len(vocabList)
    for word in words:
        if word in vocabList:
            wordVec[vocabList.index(word)] = 1
    return wordVec


def bagOfWords2Vec(vocabList, words):
    wordVec = [0] * len(vocabList)
    for word in words:
        if word in vocabList:
            wordVec[vocabList.index(word)] += 1
    return wordVec


def textParse1(line):
    line = re.sub(r'[a-zA-Z.【】0-9、。，/！…~\*\n]', '', line)
    line = jieba.lcut(line, cut_all=True)
    return [w for w in line if len(w) > 1]


wordList = []
classList = []
for i in range(127):
    wordList_s = textParse1(open(r'C:\Users\hyh\Desktop\email-zh\垃圾邮件\%d.txt' % i, encoding='utf8').read())
    wordList.append(wordList_s)
    classList.append(1)
for i in range(29):
    wordList_h = textParse1(open(r'C:\Users\hyh\Desktop\email-zh\正常邮件\%d.txt' % i, encoding='utf8').read())
    wordList.append(wordList_h)
    classList.append(0)

vocabList = creatVocabList(wordList)
trainMat = []
for words in wordList:
    trainMat.append(setOfWords2Vec(vocabList, words))

from sklearn.naive_bayes import MultinomialNB,GaussianNB

gn = GaussianNB()
gn = gn.fit(trainMat, classList)
testWords = textParse1(open(r'C:\Users\hyh\Desktop\email-zh\垃圾邮件\1.txt', encoding='utf8').read())
newWordVec = setOfWords2Vec(vocabList, testWords)
result=gn.predict(np.array(newWordVec).reshape(1, -1))
print(result)
```

# 相似度

```python
#1.直接计算相似度
import numpy as np

R = np.array([[5, 5, 0, 5], [5, 0, 3, 4], [3, 4, 0, 3], [0, 0, 5, 3], [5, 4, 4, 5], [5, 4, 5, 5]])

def cos_dis(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


for i in range(R.shape[0]):
    a = []
    for j in range(R.shape[0]):
        if i != j:
            a.append((i, j, np.round(cos_dis(R[i], R[j]), 2)))
    c = sorted(a, key=lambda x: x[2], reverse=True)


#2.降维处理

import numpy as np

R = np.array([[5, 5, 0, 5], [5, 0, 3, 4], [3, 4, 0, 3], [0, 0, 5, 3], [5, 4, 4, 5], [5, 4, 5, 5]])

def sigma_k(sigma, percentage):
    sigma2 = 0
    k = 0
    for i in sigma:
        sigma2 += i ** 2
        k += 1
        if (sigma2 / sum(sigma ** 2)).all() > percentage:
            return k


k=sigma_k(R,0.9)
U,sigma,V=np.linalg.svd(R)

u_new=U[:,:k]

#
import matplotlib.pyplot as plt

x = u_new[:, 0]
y = u_new[:, 1]
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
plt.scatter(x, y)   #散点图

txt = ['s1', 's2', 's3', 's4', 's5', 's6']
for i in range(len(x)):
    plt.annotate(txt[i], xy=(x[i] + 0.003, y[i] + 0.003))

# plt.show()


import pandas as pd
import numpy as np

data1 = pd.read_excel(r'C:\Users\hyh\Desktop\预测评分\remmand.xlsx')
data = data1.values   #把得到额数据转化为矩阵
data = data.T   #以物品来进行相似度算法，装置把物品作为行，方便操作

#data:数据   item:行  user:列
def f(data, item, user):
    sim_total = 0   #相似度总和
    sim_ = 0   #权重
    for j in range(data.shape[0]):
        if j != item:
            sim = cos_dis(data[item], data[j])
            sim_total += sim
            sim_ += sim * data[j,user]   #相似度*该位置的分数
    return sim_ / sim_total

def recommand(data, user):
    item_scores = []
    for item in range(data.shape[0]):
        if data[item,user] == 0:
            data[item, user] = f(data, item, user)
            item_scores.append((item, data[item, user]))
    item_scores = sorted(item_scores, key=lambda x: x[1], reverse=True)
    return item_scores[:3]
scores=recommand(data,2)
```

# 图像特征提取

## Hog特征提取

1. 预处理：以灰度形式读入图片，并归一化（如必要时进行裁剪，调整图片大小：crop,resize）
2. 计算梯度图像
   1. 计算每个像素点的梯度值（sobel算子）
   2. 计算水平和垂直方向的梯度幅度和梯度方向
3. 计算8*8网格的梯度
   1. 每个cell有9个bin
   2. 初始化的cell的梯度向量，求出每个cell的梯度幅值和方向
4. 16*16块归一化-------消除光线影响
5. 计算hog特征向量
6. 可视化hog

